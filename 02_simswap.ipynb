{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "580df5c7-ceb6-4202-8945-25946a98adcf",
   "metadata": {},
   "source": [
    "# 가이드 문서 확인\n",
    "### https://docs.google.com/document/d/1njuktrxhwp6hNR_CtnF1RepmrYRLPjaHWC86D1sqJpc/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f555b019-f1b2-46c1-9efd-6ab4969946ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/neuralchen/SimSwap.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59348632-5203-4811-9631-f82e0819ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/neuralchen/SimSwap/blob/main/docs/guidance/preparation.md\n",
    "# 위 페이지를 참고하여 pretrained model download 및 필요 package 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a15f72-efcc-4604-b09e-6902ac36ba5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"SimSwap\")\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb16c6e-945f-40ab-8380-7198e073d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import fractions\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from models.models import create_model\n",
    "from options.test_options import TestOptions\n",
    "from insightface_func.face_detect_crop_multi import Face_detect_crop\n",
    "from util.videoswap import video_swap\n",
    "from util.add_watermark import watermark_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc25966-212e-44d1-b6b7-1f63c793affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "transformer_Arcface = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "detransformer = transforms.Compose([\n",
    "        transforms.Normalize([0, 0, 0], [1/0.229, 1/0.224, 1/0.225]),\n",
    "        transforms.Normalize([-0.485, -0.456, -0.406], [1, 1, 1])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82869a98-4118-4cb7-845f-0079c71a4391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pics = glob.glob('/data/stf/github/simswap/imgs/*')\n",
    "vids = glob.glob('/data/stf/github/simswap/videos/*')\n",
    "\n",
    "for v in vids:\n",
    "    for p in pics:\n",
    "        #try:\n",
    "        print(f\"Processing {p} to {v}.\")\n",
    "        opt = TestOptions()\n",
    "        opt.initialize()\n",
    "        opt.parser.add_argument('-f') ## dummy arg to avoid bug\n",
    "        opt = opt.parse()\n",
    "        opt.pic_a_path = p ## or replace it with image from your own google drive\n",
    "        opt.video_path = v ## or replace it with video from your own google drive\n",
    "        v_name = v.split('/')[-1].split('.')[0]\n",
    "        p_name = p.split('/')[-1].split('.')[0]\n",
    "        opt.output_path = f'/data/stf/github/simswap/output/{v_name}_{p_name}.mp4'\n",
    "        opt.temp_path = './tmp'\n",
    "        opt.Arc_path = './arcface_model/arcface_checkpoint.tar'\n",
    "        opt.isTrain = False\n",
    "        opt.use_mask = True  ## new feature up-to-date\n",
    "\n",
    "        crop_size = opt.crop_size\n",
    "\n",
    "        torch.nn.Module.dump_patches = True\n",
    "        model = create_model(opt)\n",
    "        model.eval()\n",
    "\n",
    "        app = Face_detect_crop(name='antelope', root='./insightface_func/models')\n",
    "        app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pic_a = opt.pic_a_path\n",
    "            # img_a = Image.open(pic_a).convert('RGB')\n",
    "            img_a_whole = cv2.imread(pic_a)\n",
    "            img_a_align_crop, _ = app.get(img_a_whole,crop_size)\n",
    "            img_a_align_crop_pil = Image.fromarray(cv2.cvtColor(img_a_align_crop[0],cv2.COLOR_BGR2RGB)) \n",
    "            img_a = transformer_Arcface(img_a_align_crop_pil)\n",
    "            img_id = img_a.view(-1, img_a.shape[0], img_a.shape[1], img_a.shape[2])\n",
    "\n",
    "            # convert numpy to tensor\n",
    "            img_id = img_id.cuda()\n",
    "\n",
    "            #create latent id\n",
    "            img_id_downsample = F.interpolate(img_id, size=(112,112))\n",
    "            latend_id = model.netArc(img_id_downsample)\n",
    "            latend_id = latend_id.detach().to('cpu')\n",
    "            latend_id = latend_id/np.linalg.norm(latend_id,axis=1,keepdims=True)\n",
    "            latend_id = latend_id.to('cuda')\n",
    "\n",
    "            video_swap(opt.video_path, latend_id, model, app, opt.output_path, temp_results_dir=opt.temp_path, use_mask=opt.use_mask)\n",
    "\n",
    "        print(f\"Success {p} to {v}.\")\n",
    "        # except:\n",
    "        #     print(f\"Fail {p} to {v}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simswap",
   "language": "python",
   "name": "simswap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
